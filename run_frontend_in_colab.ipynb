{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bCPDHP7zTQJZ",
    "outputId": "c75f4593-706c-4d2d-ddca-b3261fb845e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download nltk stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAt6KT8xOgHH",
    "outputId": "0c67a9ce-b162-49b9-b614-8c6d895b9fa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/106.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m92.2/106.6 kB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m106.6/106.6 kB\u001B[0m \u001B[31m1.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 1.29.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.43.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "# Install a particular version of `google-cloud-storage` because (oddly enough)\n",
    "# the  version on Colab and GCP is old. A dependency error below is okay.\n",
    "!pip install -q google-cloud-storage==1.43.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-oKFly5jFLFn"
   },
   "outputs": [],
   "source": [
    "# authenticate below for Google Storage access as needed\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- build id2title from parquet on gcs and upload it back ---\n",
    "\n",
    "!pip -q install gcsfs pyarrow pandas\n",
    "\n",
    "import pandas as pd\n",
    "import gcsfs\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "BUCKET = \"information-retrival-ex3\"\n",
    "PATTERN = f\"{BUCKET}/multistream*_preprocessed.parquet\"   # gcsfs uses \"bucket/path\" (no gs://)\n",
    "OUT_LOCAL = \"id2title.pickle\"\n",
    "OUT_GCS = f\"gs://{BUCKET}/artifacts/id2title.pickle\"\n",
    "\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "paths = sorted(fs.glob(PATTERN))\n",
    "\n",
    "print(\"found parquet files:\", len(paths))\n",
    "print(\"first 3:\", paths[:3])\n",
    "\n",
    "# ---- detect actual column names (schema may vary) ----\n",
    "sample_path = \"gs://\" + paths[0]\n",
    "sample_df = pd.read_parquet(sample_path, engine=\"pyarrow\")\n",
    "cols = set(sample_df.columns)\n",
    "print(\"sample columns:\", sample_df.columns.tolist())\n",
    "\n",
    "# common possibilities\n",
    "ID_CANDIDATES = [\"id\", \"doc_id\", \"wiki_id\"]\n",
    "TITLE_CANDIDATES = [\"title\", \"doc_title\", \"document_title\"]\n",
    "\n",
    "id_col = next((c for c in ID_CANDIDATES if c in cols), None)\n",
    "title_col = next((c for c in TITLE_CANDIDATES if c in cols), None)\n",
    "\n",
    "if id_col is None or title_col is None:\n",
    "    raise ValueError(f\"could not find id/title columns in parquet. columns={sample_df.columns.tolist()}\")\n",
    "\n",
    "print(\"using:\", id_col, title_col)\n",
    "\n",
    "# ---- build mapping ----\n",
    "id2title = {}\n",
    "\n",
    "for i, p in enumerate(paths, 1):\n",
    "    df = pd.read_parquet(\"gs://\" + p, columns=[id_col, title_col], engine=\"pyarrow\")\n",
    "\n",
    "    ids = df[id_col].astype(\"int64\").tolist()\n",
    "    titles = df[title_col].astype(str).tolist()\n",
    "    id2title.update(dict(zip(ids, titles)))\n",
    "\n",
    "    if i % 5 == 0 or i == len(paths):\n",
    "        print(f\"[{i}/{len(paths)}] total mapped ids so far:\", len(id2title))\n",
    "\n",
    "with open(OUT_LOCAL, \"wb\") as f:\n",
    "    pickle.dump(id2title, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"saved:\", OUT_LOCAL, \"size bytes:\", os.path.getsize(OUT_LOCAL))\n",
    "\n",
    "!gsutil -m cp {OUT_LOCAL} {OUT_GCS}\n",
    "!gsutil ls -l {OUT_GCS}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "#\n",
    "# !pip -q install gcsfs pyarrow pandas"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re, math, os, pickle, hashlib\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import gcsfs\n",
    "\n",
    "# upload your inverted_index_gcp.py to colab (Files panel) before running this\n",
    "from inverted_index_gcp import InvertedIndex\n",
    "\n",
    "# tokenizer like your project\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "corpus_stopwords = {\"category\", \"references\", \"also\", \"links\", \"extenal\", \"see\", \"thumb\"}\n",
    "english_stopwords = set(stopwords.words(\"english\"))\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "\n",
    "def tokenize(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = [m.group() for m in RE_WORD.finditer(text.lower())]\n",
    "    return [t for t in tokens if t not in all_stopwords]# Build TITLE inverted index"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "BUCKET = \"information-retrival-ex3\"\n",
    "PATTERN = f\"{BUCKET}/multistream*_preprocessed.parquet\"  # gcsfs style (no gs://)\n",
    "\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "paths = sorted(fs.glob(PATTERN))\n",
    "print(\"parquet shards:\", len(paths))\n",
    "print(\"first 3:\", paths[:3])\n",
    "\n",
    "# build in-memory title index\n",
    "title_index = InvertedIndex()\n",
    "total_docs = 0\n",
    "\n",
    "for i, p in enumerate(paths, 1):\n",
    "    df = pd.read_parquet(\"gs://\" + p, columns=[\"id\", \"title\"], engine=\"pyarrow\")\n",
    "\n",
    "    ids = df[\"id\"].astype(\"int64\").tolist()\n",
    "    titles = df[\"title\"].astype(str).tolist()\n",
    "\n",
    "    for doc_id, title in zip(ids, titles):\n",
    "        toks = tokenize(title)\n",
    "        if toks:\n",
    "            title_index.add_doc(int(doc_id), toks)\n",
    "\n",
    "    total_docs += len(ids)\n",
    "    if i % 5 == 0 or i == len(paths):\n",
    "        print(f\"[{i}/{len(paths)}] docs seen: {total_docs:,} | vocab so far: {len(title_index.df):,}\")\n",
    "\n",
    "print(\"done building title index.\")\n",
    "print(\"final docs seen:\", total_docs)\n",
    "print(\"final vocab:\", len(title_index.df))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = \"title_postings_gcp\"\n",
    "INDEX_NAME = \"title_index\"\n",
    "N_BUCKETS = 124  # common setting in the course\n",
    "\n",
    "# clean local folder\n",
    "if os.path.exists(OUT_DIR):\n",
    "    shutil.rmtree(OUT_DIR)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def bucket_id_for_term(term: str, n_buckets: int = N_BUCKETS) -> str:\n",
    "    # deterministic bucket (NOT python hash)\n",
    "    h = hashlib.md5(term.encode(\"utf-8\")).hexdigest()\n",
    "    return str(int(h, 16) % n_buckets).zfill(3)\n",
    "\n",
    "# group terms into buckets: bucket_id -> [(term, posting_list)]\n",
    "bucket_to_list = defaultdict(list)\n",
    "\n",
    "# IMPORTANT: this uses the in-memory posting lists created during add_doc()\n",
    "# InvertedIndex stores them in _posting_list while building.\n",
    "posting_dict = title_index._posting_list  # internal\n",
    "\n",
    "print(\"bucketing terms...\")\n",
    "for t, pl in posting_dict.items():\n",
    "    bid = bucket_id_for_term(t)\n",
    "    # sort posting list by doc_id (good practice)\n",
    "    pl_sorted = sorted(pl, key=lambda x: x[0])\n",
    "    bucket_to_list[bid].append((t, pl_sorted))\n",
    "\n",
    "print(\"writing posting bins...\")\n",
    "for bid in sorted(bucket_to_list.keys()):\n",
    "    InvertedIndex.write_a_posting_list((bid, bucket_to_list[bid]), OUT_DIR, bucket_name=None)\n",
    "\n",
    "print(\"merging posting_locs into the index object...\")\n",
    "title_index.posting_locs = defaultdict(list)\n",
    "for bid in sorted(bucket_to_list.keys()):\n",
    "    locs_path = os.path.join(OUT_DIR, f\"{bid}_posting_locs.pickle\")\n",
    "    with open(locs_path, \"rb\") as f:\n",
    "        d = pickle.load(f)\n",
    "    for term, locs in d.items():\n",
    "        title_index.posting_locs[term].extend(locs)\n",
    "\n",
    "# now we can drop the heavy in-memory postings before pickling the index\n",
    "# (InvertedIndex.__getstate__ may do this anyway, but we do it explicitly)\n",
    "del title_index._posting_list\n",
    "\n",
    "print(\"writing title_index globals (df, term_total, posting_locs)...\")\n",
    "title_index.write_index(OUT_DIR, INDEX_NAME, bucket_name=None)\n",
    "\n",
    "print(\"local output folder size (rough):\")\n",
    "!du -sh {OUT_DIR}\n",
    "\n",
    "# upload to gcs\n",
    "DEST = f\"gs://{BUCKET}/{OUT_DIR}/\"\n",
    "print(\"uploading to:\", DEST)\n",
    "!gsutil -m rm -r {DEST} 2>/dev/null || true\n",
    "!gsutil -m cp -r {OUT_DIR} {DEST}\n",
    "\n",
    "print(\"verify on gcs:\")\n",
    "!gsutil ls {DEST} | head\n",
    "!gsutil ls {DEST} | grep -E \"{INDEX_NAME}\\.(pkl|pickle)$\" || true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dW0y91OVu5J"
   },
   "source": [
    "# Run the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7opNkV6uRHIv"
   },
   "outputs": [],
   "source": [
    "# you need to upload your implementation of search_app.py\n",
    "import search_frontend as se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTGXXYEXV5l8"
   },
   "outputs": [],
   "source": [
    "# uncomment the code below and execute to reload the module when you make\n",
    "# changes to search_frontend.py (after you upload again).\n",
    "# import importlib\n",
    "# importlib.reload(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "k7K7oFlfVhm5",
    "outputId": "8f31e64f-e631-4dca-a182-ea283cf9ecf0"
   },
   "outputs": [],
   "source": [
    "# find Colab's public URL\n",
    "from google.colab.output import eval_js\n",
    "server_url = eval_js(\"google.colab.kernel.proxyPort(5000)\")\n",
    "print(f\"\"\"Test your search engine by navigating to\n",
    "{server_url}search?query=hello+world\n",
    "This URL is only accessible from the same browser session. In other words, this\n",
    "will not be accessible from a different machine, browser, or incognito session.\n",
    "\"\"\")\n",
    "\n",
    "# Uncomment the following line of code to run the frontend in the main\n",
    "# process and wait for HTTP requests (colab will hang). The debug parameter\n",
    "# lets you see incoming requests and get debug print outs if exceptions occur.\n",
    "# se.run(debug=False, use_reloader=False)\n",
    "\n",
    "# Alternatively, the next few lines run the frontend in a background process.\n",
    "# Just don't forget to terminate the process when you update your search engine\n",
    "# or want to reload it.\n",
    "import multiprocessin, time\n",
    "proc = multiprocessing.Process(target=se.run, \n",
    "                               kwargs={\"debug\": True, \"use_reloader\": False, \n",
    "                                       \"host\": \"0.0.0.0\", \"port\": 5000})\n",
    "proc.start()\n",
    "\n",
    "time.sleep(1) # give Flask time to boot\n",
    "\n",
    "from google.colab.output import eval_js\n",
    "server_url = eval_js(\"google.colab.kernel.proxyPort(5000)\")\n",
    "\n",
    "print(f\"Open this URL:\\n{server_url}/search?query=hello+world\")\n",
    "# Use proc.terminate() to stop the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Na0MC_1nzDbi"
   },
   "source": [
    "# Testing your app\n",
    "\n",
    "Once your app is running you can query it. You can simply do that by clicking on the URL printed above (the one looking like https://XXXXX-5000-colab.googleusercontent.com/search?query=hello+world or by issuing an HTTP request through code (from colab).\n",
    "\n",
    "The code below shows how to issue a query from python. This is also how our testing code will issue queries to your search engine, so make sure to test your search engine this way after you deploy it to GCP and before submission. Command line instructions for deploying your search engine to GCP are available at `run_frontend_in_gcp.sh`. Note that we will not only issue training queries to your search engine, but also test queries, i.e. queries that you've never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EM5ePrRHojbG"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('queries_train.json', 'rt') as f:\n",
    "  queries = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWimZWCOy3Ei"
   },
   "outputs": [],
   "source": [
    "def average_precision(true_list, predicted_list, k=40):\n",
    "    true_set = frozenset(true_list)\n",
    "    predicted_list = predicted_list[:k]\n",
    "    precisions = []\n",
    "    for i,doc_id in enumerate(predicted_list):\n",
    "        if doc_id in true_set:\n",
    "            prec = (len(precisions)+1) / (i+1)\n",
    "            precisions.append(prec)\n",
    "    if len(precisions) == 0:\n",
    "        return 0.0\n",
    "    return round(sum(precisions)/len(precisions),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geHKyFB4xkBe"
   },
   "outputs": [],
   "source": [
    "def precision_at_k(true_list, predicted_list, k):\n",
    "    true_set = frozenset(true_list)\n",
    "    predicted_list = predicted_list[:k]\n",
    "    if len(predicted_list) == 0:\n",
    "        return 0.0\n",
    "    return round(len([1 for doc_id in predicted_list if doc_id in true_set]) / len(predicted_list), 3)\n",
    "def recall_at_k(true_list, predicted_list, k):\n",
    "    true_set = frozenset(true_list)\n",
    "    predicted_list = predicted_list[:k]\n",
    "    if len(true_set) < 1:\n",
    "        return 1.0\n",
    "    return round(len([1 for doc_id in predicted_list if doc_id in true_set]) / len(true_set), 3)\n",
    "def f1_at_k(true_list, predicted_list, k):\n",
    "    p = precision_at_k(true_list, predicted_list, k)\n",
    "    r = recall_at_k(true_list, predicted_list, k)\n",
    "    if p == 0.0 or r == 0.0:\n",
    "        return 0.0\n",
    "    return round(2.0 / (1.0/p + 1.0/r), 3)\n",
    "def results_quality(true_list, predicted_list):\n",
    "    p5 = precision_at_k(true_list, predicted_list, 5)\n",
    "    f1_30 = f1_at_k(true_list, predicted_list, 30)\n",
    "    if p5 == 0.0 or f1_30 == 0.0:\n",
    "        return 0.0\n",
    "    return round(2.0 / (1.0/p5 + 1.0/f1_30), 3)\n",
    "\n",
    "assert precision_at_k(range(10), [1,2,3] , 2) == 1.0\n",
    "assert recall_at_k(   range(10), [10,5,3], 2) == 0.1\n",
    "assert precision_at_k(range(10), []      , 2) == 0.0\n",
    "assert precision_at_k([],        [1,2,3],  5) == 0.0\n",
    "assert recall_at_k(   [],        [10,5,3], 2) == 1.0\n",
    "assert recall_at_k(   range(10), [],       2) == 0.0\n",
    "assert f1_at_k(       [],        [1,2,3],  5) == 0.0\n",
    "assert f1_at_k(       range(10), [],       2) == 0.0\n",
    "assert f1_at_k(       range(10), [0,1,2],  2) == 0.333\n",
    "assert f1_at_k(       range(50), range(5), 30) == 0.182\n",
    "assert f1_at_k(       range(50), range(10), 30) == 0.333\n",
    "assert f1_at_k(       range(50), range(30), 30) == 0.75\n",
    "assert results_quality(range(50), range(5))  == 0.308\n",
    "assert results_quality(range(50), range(10)) == 0.5\n",
    "assert results_quality(range(50), range(30)) == 0.857\n",
    "assert results_quality(range(50), [-1]*5 + list(range(5,30))) == 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYmNTq9u0ChK"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import time\n",
    "# In GCP the public URL for your engine should look like this:\n",
    "# url = 'http://35.232.59.3:8080'\n",
    "# In colab, we are going to send HTTP requests to localhost (127.0.0.1)\n",
    "# and direct them to port where the server is listening (5000).\n",
    "url = 'http://127.0.0.1:5000'\n",
    "\n",
    "qs_res = []\n",
    "for q, true_wids in queries.items():\n",
    "  duration, ap = None, None\n",
    "  t_start = time()\n",
    "  try:\n",
    "    res = requests.get(url + '/search', {'query': q}, timeout=35)\n",
    "    duration = time() - t_start\n",
    "    if res.status_code == 200:\n",
    "      pred_wids, _ = zip(*res.json())\n",
    "      rq = results_quality(true_wids, pred_wids)\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  qs_res.append((q, duration, rq))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
